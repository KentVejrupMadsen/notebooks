{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzlKzwdLMEkO"
      },
      "outputs": [],
      "source": [
        "# @title Installation of required packages with pip. { run: \"auto\", display-mode: \"form\" }\n",
        "location_to_requirements: str = 'requirements.txt'\n",
        "download_requirements_from: str = 'https://gist.githubusercontent.com/KentVejrupMadsen/d0400cf3a07bbcf4f8abd9b2ea087d86/raw/c0dcbb239892e5844ca0edf3d5ab696e5a740742/requirements.txt'\n",
        "!wget {download_requirements_from} -O {location_to_requirements}\n",
        "%pip install -r {location_to_requirements}\n",
        "\n",
        "%pip install pycocotools\n",
        "%pip install keras-cv keras keras-core tensorflow --upgrade -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def zero() -> int:\n",
        "  return 0\n",
        "\n",
        "def one() -> int:\n",
        "  return 1\n",
        "\n",
        "def two() -> int:\n",
        "  return 2\n",
        "\n",
        "def three() -> int:\n",
        "  return 3\n",
        "\n",
        "def four() -> int:\n",
        "  return 4\n",
        "\n",
        "def five() -> int:\n",
        "  return 5\n",
        "\n",
        "def six() -> int:\n",
        "  return 6\n",
        "\n",
        "def seven() -> int:\n",
        "  return 7\n",
        "\n",
        "def eight() -> int:\n",
        "  return 8\n",
        "\n",
        "def nine() -> int:\n",
        "  return 9\n",
        "\n",
        "def max_limit_for_rgb_color() -> int:\n",
        "  return 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-28 19:43:33.298013: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-01-28 19:43:33.331728: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-28 19:43:33.331756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-28 19:43:33.333238: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-28 19:43:33.341085: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-28 19:43:33.975073: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "from numpy \\\n",
        "  import array\n",
        "\n",
        "import tensorflow\n",
        "import keras\n",
        "\n",
        "from keras.utils                \\\n",
        "  import                        \\\n",
        "  load_img,                     \\\n",
        "  img_to_array\n",
        "\n",
        "from keras.optimizers           \\\n",
        "  import                        \\\n",
        "  SGD,                          \\\n",
        "  Adam\n",
        "\n",
        "from keras \\\n",
        "  import Model\n",
        "\n",
        "from keras.models               \\\n",
        "  import Sequential\n",
        "\n",
        "from keras.layers               \\\n",
        "  import Rescaling\n",
        "\n",
        "from keras.optimizers.schedules \\\n",
        "  import ExponentialDecay\n",
        "\n",
        "from keras.utils                \\\n",
        "  import plot_model\n",
        "\n",
        "from keras.models               \\\n",
        "  import load_model\n",
        "\n",
        "from keras.callbacks            \\\n",
        "  import                        \\\n",
        "  Callback,                     \\\n",
        "  TensorBoard\n",
        "\n",
        "from keras.mixed_precision \\\n",
        "  import set_global_policy\n",
        "import keras_cv\n",
        "\n",
        "from keras_cv   \\\n",
        "  import        \\\n",
        "  bounding_box, \\\n",
        "  visualization\n",
        "\n",
        "from keras_cv.metrics \\\n",
        "  import BoxCOCOMetrics\n",
        "\n",
        "from keras_cv   \\\n",
        "  import layers \\\n",
        "  as cv_layers\n",
        "\n",
        "from keras_cv.models  \\\n",
        "  import              \\\n",
        "  YOLOV8Backbone,     \\\n",
        "  YOLOV8Detector\n",
        "\n",
        "from keras_cv.layers \\\n",
        "  import Resizing\n",
        "\n",
        "from tensorflow.ragged \\\n",
        "  import constant\n",
        "\n",
        "from tensorflow \\\n",
        "  import get_logger\n",
        "\n",
        "from tensorflow.config \\\n",
        "  import list_physical_devices\n",
        "\n",
        "from tensorflow.config.experimental \\\n",
        "  import set_memory_growth\n",
        "\n",
        "from tensorflow.data \\\n",
        "  import Dataset\n",
        "\n",
        "from tqdm.auto \\\n",
        "  import tqdm\n",
        "\n",
        "import wandb\n",
        "\n",
        "from wandb  \\\n",
        "  import    \\\n",
        "  init\n",
        "\n",
        "from wandb.integration.keras \\\n",
        "  import WandbMetricsLogger\n",
        "\n",
        "from os \\\n",
        "  import (\n",
        "    environ, \n",
        "    mkdir\n",
        "  )\n",
        "\n",
        "from os.path \\\n",
        "  import isdir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_hd_width: int = 1920 \n",
        "full_hd_height: int = 1080 \n",
        "\n",
        "scale_by_size: float = 2 \n",
        "\n",
        "has_fhd_width: int = int(\n",
        "    full_hd_width\n",
        "    /\n",
        "    scale_by_size\n",
        ")\n",
        "\n",
        "has_fhd_height: int = int(\n",
        "    full_hd_height\n",
        "    /\n",
        "    scale_by_size\n",
        ")\n",
        "\n",
        "show_summary_of_model: bool = False \n",
        "\n",
        "is_dataset_saved: bool = False\n",
        "stop_when_done: bool = False\n",
        "\n",
        "model_name: str = 'mjoelner' \n",
        "wandb_label_model: str = 'model' \n",
        "\n",
        "location_for_tensorboard = '/tmp/tensorboard/'\n",
        "\n",
        "notebook_name: str = 'Targeting System, YOLO, Mjoelner.ipynb' \n",
        "\n",
        "location_for_sample_data: str = '/content/sample_data' \n",
        "\n",
        "is_google_colab_platform: bool = False\n",
        "use_of_multiprocessing: bool = True \n",
        "delete_sample_data: bool = False\n",
        "shutdown_on_finish: bool = True \n",
        "\n",
        "tensorflow_verbosity: int = 1\n",
        "\n",
        "use_of_workers: int = 5\n",
        "image_channels: int = 3\n",
        "\n",
        "use_of_wandb: bool = True \n",
        "\n",
        "wandb_to_save_code: bool = False \n",
        "wandb_to_sync_tensorboard: bool = True \n",
        "\n",
        "wandb_project_name: str = 'Spectre' \n",
        "wandb_entity_name: str = 'designermadsen'\n",
        "\n",
        "wandb_job_type: str = 'Training'\n",
        "wandb_group: str = 'Laptop'\n",
        "\n",
        "wandb_allow_reinitialisation: bool = True \n",
        "\n",
        "finish_wandb_when_done: bool = True\n",
        "\n",
        "wandb_api_key: str = '2fc089ae788974434469d090de2a4ce894aa2843' \n",
        "wandb_project_tags: str = 'YOLOv8, Laptop, ASUS, GeForce, RTX-4050, Targeting-System, Tensorflow, Keras, Keras-CV, Training, Experiment, NVIDIA, CUDA, AdamW' \n",
        "wandb_tags: list = wandb_project_tags.split(\n",
        "  ','\n",
        ")\n",
        "wandb_tags.sort()\n",
        "\n",
        "hp_initial_learning_rate: float = 0.001\n",
        "\n",
        "hp_decay_rate: float = 0.8624242424242429\n",
        "hp_decay_steps: int = 550 \n",
        "\n",
        "iterate_training: int = 4\n",
        "hp_epochs: int = 11\n",
        "\n",
        "hp_batch_size: int = 2\n",
        "hp_global_clip_at: float = 25.0\n",
        "\n",
        "download_dataset_at: str | None = None \n",
        "\n",
        "dataset_split_at: float = 0.25 \n",
        "\n",
        "use_resize: bool = True \n",
        "use_jitter_resize: bool = False \n",
        "use_random_flip: bool = True \n",
        "\n",
        "use_random_contrast: bool = False \n",
        "use_random_brightness: bool = False \n",
        "use_grayscale_augmentation: bool = False \n",
        "use_grid_mask: bool = False \n",
        "\n",
        "tune_value: int = hp_batch_size\n",
        "\n",
        "scale_boundary_lowest: float    = 0.25 \n",
        "scale_boundary_highest: float   = 1.0\n",
        "\n",
        "target_input_width: int     = has_fhd_width\n",
        "target_input_height: int    = has_fhd_height\n",
        "target_input_channels: int  = image_channels\n",
        "\n",
        "model_width: int    = has_fhd_width\n",
        "model_height: int   = has_fhd_height\n",
        "model_channels: int = image_channels\n",
        "\n",
        "batch_size: int = hp_batch_size\n",
        "\n",
        "graph_model: bool = False\n",
        "global_clip_at: float = hp_global_clip_at\n",
        "\n",
        "shuffle_sample_by: int = batch_size * 4\n",
        "\n",
        "use_coco_metrics: bool = True\n",
        "load_model_from: str | None = None\n",
        "\n",
        "save_final_model_at: str = '/tmp/model.final.keras'\n",
        "\n",
        "set_memory_growth_to: bool = False\n",
        "set_memory_to_fixed: bool = True\n",
        "\n",
        "tensorflow_is_to_log_critical_information_only: bool = True\n",
        "wandb_is_to_log_critical_information_only: bool = True "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_variable_none(\n",
        "    variable\n",
        ") -> bool:\n",
        "    return variable is None\n",
        "\n",
        "def is_integer_zero(\n",
        "    variable: int\n",
        ") -> bool:\n",
        "    return variable == zero()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EK6aCXZnGq9J"
      },
      "outputs": [],
      "source": [
        "sorted_list: list = list()\n",
        "\n",
        "for idx in range(len(wandb_tags)):\n",
        "  result = wandb_tags[idx].replace(\n",
        "    ' ', \n",
        "    ''\n",
        "  )\n",
        "\n",
        "  if not result == '':\n",
        "    sorted_list.append(\n",
        "      result\n",
        "    )\n",
        "\n",
        "wandb_tags = sorted_list.copy()\n",
        "\n",
        "del sorted_list\n",
        "\n",
        "if not isdir(\n",
        "  location_for_tensorboard\n",
        "):\n",
        "  mkdir(\n",
        "    location_for_tensorboard\n",
        "  )\n",
        "\n",
        "if (\n",
        "    is_variable_none(\n",
        "      use_of_workers\n",
        "    )\n",
        "    or\n",
        "    is_integer_zero(\n",
        "      use_of_workers\n",
        "    )\n",
        "):\n",
        "  from multiprocessing \\\n",
        "    import cpu_count\n",
        "  use_of_workers = cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Rq7muAv2_8T0"
      },
      "outputs": [],
      "source": [
        "def get_image_channels() -> int:\n",
        "  global image_channels\n",
        "  return image_channels\n",
        "\n",
        "def set_image_channels(\n",
        "  value: int\n",
        ") -> None:\n",
        "  global image_channels\n",
        "  image_channels = value\n",
        "  \n",
        "def get_tune_value() -> int:\n",
        "  global tune_value\n",
        "  return tune_value\n",
        "\n",
        "def get_save_cache_at_for_evaluation() -> str:\n",
        "  global save_cache_at_for_evaluation\n",
        "  return save_cache_at_for_evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NkwtZAuFroWO"
      },
      "outputs": [],
      "source": [
        "def gpu_label() -> str:\n",
        "  return 'GPU'\n",
        "\n",
        "def label_dataset_annotations() -> str:\n",
        "  return 'annotations'\n",
        "\n",
        "def label_dataset_images() -> str:\n",
        "  return 'images'\n",
        "\n",
        "def label_map() -> str:\n",
        "  return 'MaP'\n",
        "\n",
        "def get_label_name() -> str:\n",
        "  label_name: str = 'name'\n",
        "  return label_name\n",
        "\n",
        "def get_label_object() -> str:\n",
        "  label_object: str = 'object'\n",
        "  return label_object\n",
        "\n",
        "def get_label_bounding_box() -> str:\n",
        "  label_bounding_box: str = 'bndbox'\n",
        "  return label_bounding_box\n",
        "\n",
        "def get_label_x_minimum() -> str:\n",
        "  label_x_minimum: str = 'xmin'\n",
        "  return label_x_minimum\n",
        "\n",
        "def get_label_y_minimum() -> str:\n",
        "  label_y_minimum: str = 'ymin'\n",
        "  return label_y_minimum\n",
        "\n",
        "def get_label_x_maximum() -> str:\n",
        "  label_x_maximum: str = 'xmax'\n",
        "  return label_x_maximum\n",
        "\n",
        "def get_label_y_maximum() -> str:\n",
        "  label_y_maximum: str = 'ymax'\n",
        "  return label_y_maximum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OtMLjUIH6UJ9"
      },
      "outputs": [],
      "source": [
        "def is_equal_to_zero(\n",
        "    value: int | float\n",
        ") -> bool:\n",
        "  if isinstance(\n",
        "      value,\n",
        "      float\n",
        "  ):\n",
        "    return value == float(\n",
        "        zero()\n",
        "    )\n",
        "\n",
        "  return value == zero()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHaVvf7OCz-w"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zm54miteBAlh"
      },
      "outputs": [],
      "source": [
        "decay_steps_low_point: int = 10\n",
        "decay_steps_high_point: int = 500 \n",
        "\n",
        "decay_rate_low_point: float = 0.2387111124 \n",
        "decay_rate_high_point: float = 0.9940009441 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "y3QoT9aV0hnb"
      },
      "outputs": [],
      "source": [
        "def generate_decay_steps() -> int:\n",
        "  global                    \\\n",
        "    decay_steps_low_point,  \\\n",
        "    decay_steps_high_point\n",
        "\n",
        "  from random \\\n",
        "    import SystemRandom\n",
        "\n",
        "  low_point: int = decay_steps_low_point\n",
        "  high_point: int = decay_steps_high_point\n",
        "\n",
        "  return SystemRandom().randint(\n",
        "      low_point,\n",
        "      high_point\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YgOibfzf-Xn2"
      },
      "outputs": [],
      "source": [
        "def generate_decay_rate() -> float:\n",
        "  global                  \\\n",
        "    decay_rate_low_point, \\\n",
        "    decay_rate_high_point\n",
        "  \n",
        "  from random \\\n",
        "    import SystemRandom\n",
        "\n",
        "  low_point: float  = decay_rate_low_point\n",
        "  high_point: float = decay_rate_high_point\n",
        "\n",
        "  return SystemRandom().uniform(\n",
        "      low_point,\n",
        "      high_point\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RLSie3-BOrU3"
      },
      "outputs": [],
      "source": [
        "bounding_format = 'xyxy' \n",
        "bounding_format = bounding_format.lower()\n",
        "\n",
        "def get_bounding_format() -> str:\n",
        "  global bounding_format\n",
        "  return bounding_format\n",
        "\n",
        "def set_bounding_format(\n",
        "  value: str\n",
        ") -> None:\n",
        "  global bounding_format\n",
        "  bounding_format = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "p4pPB7ND3O2U"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "    (hp_decay_rate is None)\n",
        "    or\n",
        "    (hp_decay_rate == float(zero()))\n",
        "):\n",
        "  hp_decay_rate = generate_decay_rate()\n",
        "\n",
        "if(\n",
        "    (hp_decay_steps is None)\n",
        "    or\n",
        "    (hp_decay_steps == zero())\n",
        "):\n",
        "  hp_decay_steps = generate_decay_steps()\n",
        "\n",
        "if(\n",
        "    (hp_epochs is None)\n",
        "    or\n",
        "    (hp_epochs == zero())\n",
        "):\n",
        "  hp_epochs = 11\n",
        "\n",
        "if(\n",
        "    (hp_global_clip_at is None)\n",
        "    or\n",
        "    (hp_global_clip_at == float(zero()))\n",
        "):\n",
        "  hp_global_clip_at = 10.0\n",
        "\n",
        "if(\n",
        "    (hp_initial_learning_rate is None)\n",
        "    or\n",
        "    (\n",
        "      hp_initial_learning_rate\n",
        "      ==\n",
        "      float(\n",
        "        zero()\n",
        "      )\n",
        "    )\n",
        "):\n",
        "  hp_initial_learning_rate = 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7en12DpN2XDO"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "  not(download_dataset_at is None)\n",
        "):\n",
        "  download_dataset_at = download_dataset_at.replace(\n",
        "    ' ',\n",
        "    ''\n",
        "  )\n",
        "\n",
        "is_to_select_dataset: bool = (\n",
        "    not (download_dataset_at is None)\n",
        "    or\n",
        "    (download_dataset_at == '')\n",
        ")\n",
        "\n",
        "if is_to_select_dataset:\n",
        "  download_links: list = list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0F4Qo8fQt9Fh"
      },
      "outputs": [],
      "source": [
        "if is_to_select_dataset:\n",
        "  from random \\\n",
        "    import SystemRandom\n",
        "  \n",
        "  download_links.sort()\n",
        "\n",
        "  min_random_dl: int = zero()\n",
        "  max_random_dl: int = len(download_links) - one()\n",
        "\n",
        "  download_dataset_at = download_links[\n",
        "    SystemRandom().randint(\n",
        "      min_random_dl,\n",
        "      max_random_dl\n",
        "    )\n",
        "  ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0zwQGq4CxQM"
      },
      "source": [
        "# Run Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "mDx_6Mi7i0uF"
      },
      "outputs": [],
      "source": [
        "def is_to_be_replace(\n",
        "    class_label: str\n",
        ") -> str:\n",
        "  return_label: str = class_label\n",
        "  return return_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1fPElyVVi2Yc"
      },
      "outputs": [],
      "source": [
        "def normalise_token(\n",
        "    class_label: str\n",
        ") -> str:\n",
        "  return class_label.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7ovIXdrTilMN"
      },
      "outputs": [],
      "source": [
        "def find_class_name(\n",
        "  object\n",
        "):\n",
        "  cls = str(\n",
        "    object.find(\n",
        "      get_label_name()\n",
        "    ).text\n",
        "  )\n",
        "\n",
        "  cls = is_to_be_replace(\n",
        "    cls\n",
        "  )\n",
        "  cls = normalise_token(\n",
        "    cls\n",
        "  )\n",
        "\n",
        "  return cls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "rT_PNEdyg0NI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find Mjoelner - Training Only.ipynb.\n"
          ]
        }
      ],
      "source": [
        "notebook_name = 'Mjoelner - Training Only.ipynb'\n",
        "\n",
        "environ['WANDB_API_KEY'] = wandb_api_key\n",
        "environ['WANDB_NOTEBOOK_NAME'] = notebook_name\n",
        "\n",
        "is_wandb_silent: bool = True \n",
        "environ['WANDB_SILENT'] = str(is_wandb_silent)\n",
        "environ['WANDB_SHOW_RUN'] = str(False)\n",
        "\n",
        "environ['WANDB_DISABLE_GIT'] = str(True)\n",
        "\n",
        "if use_of_wandb:\n",
        "  wandb.login()\n",
        "\n",
        "if delete_sample_data:\n",
        "  if isdir(\n",
        "      location_for_sample_data\n",
        "  ):\n",
        "    !rm {location_for_sample_data} -R\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AsR_ybOE7Ngh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-28 19:43:38.106499: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-28 19:43:38.144377: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-28 19:43:38.144417: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n"
          ]
        }
      ],
      "source": [
        "from logging import CRITICAL\n",
        "import logging\n",
        "\n",
        "use_devices: list = list()\n",
        "\n",
        "for device in list_physical_devices(\n",
        "    gpu_label()\n",
        "):\n",
        "  try:\n",
        "    set_memory_growth(\n",
        "        device,\n",
        "        set_memory_growth_to\n",
        "    )\n",
        "\n",
        "    use_devices.append(\n",
        "        device\n",
        "    )\n",
        "  except:\n",
        "    print(\n",
        "      'Invalid device'\n",
        "    )\n",
        "\n",
        "if tensorflow_is_to_log_critical_information_only:\n",
        "  get_logger().setLevel(\n",
        "      CRITICAL\n",
        "  )\n",
        "\n",
        "if wandb_is_to_log_critical_information_only:\n",
        "  wandb_logger = logging.getLogger(\n",
        "      'wandb'\n",
        "  )\n",
        "\n",
        "  wandb_logger.setLevel(\n",
        "      CRITICAL\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "physical_devices = list_physical_devices(\n",
        "    gpu_label()\n",
        ")\n",
        "\n",
        "nvidia_maximum_limit_for_gpu_memory: int = 5950\n",
        "\n",
        "for gpu in physical_devices:\n",
        "    try:\n",
        "        for gpu in physical_devices:\n",
        "            tensorflow.config.set_logical_device_configuration(\n",
        "                gpu, \n",
        "                [\n",
        "                    tensorflow.config.LogicalDeviceConfiguration(\n",
        "                        memory_limit = nvidia_maximum_limit_for_gpu_memory\n",
        "                    )\n",
        "                ]\n",
        "            )\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5F4oQMHOvrj"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GfSBjkqERdQ"
      },
      "source": [
        "### Randomizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os     \\\n",
        "  import    \\\n",
        "  listdir,  \\\n",
        "  mkdir,    \\\n",
        "  environ,  \\\n",
        "  remove\n",
        "\n",
        "from os.path  \\\n",
        "  import      \\\n",
        "  isdir,      \\\n",
        "  join,       \\\n",
        "  isfile\n",
        "\n",
        "from random \\\n",
        "  import SystemRandom\n",
        "\n",
        "import xml.etree.ElementTree \\\n",
        "  as ET\n",
        "\n",
        "import logging\n",
        "\n",
        "from logging \\\n",
        "  import CRITICAL\n",
        "\n",
        "from tempfile \\\n",
        "  import gettempdir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lOpdyF0DKxSq"
      },
      "outputs": [],
      "source": [
        "dataset_name: str = 'TMOD'\n",
        "dataset_directory_name: str = 'dataset-500'\n",
        "temporary_destination: str = '/tmp'\n",
        "\n",
        "dataset_compressed_destination: str = join(\n",
        "    temporary_destination,\n",
        "    'dataset_folder.zip'\n",
        ")\n",
        "\n",
        "destination_to_dataset: str = join(\n",
        "    temporary_destination,\n",
        "    dataset_name\n",
        ")\n",
        "\n",
        "move_to: str = join(\n",
        "    temporary_destination,\n",
        "    dataset_directory_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-MospjSoOrVL"
      },
      "outputs": [],
      "source": [
        "project_name: str = 'id ' + '(' + wandb_group.capitalize() + ')'\n",
        "\n",
        "epoch: int = hp_epochs\n",
        "\n",
        "is_to_fork_model: bool = True\n",
        "url_to_dataset: str = download_dataset_at\n",
        "\n",
        "labels_mapped: list | dict = list()\n",
        "\n",
        "is_tensorboard_initialised: bool  = False\n",
        "is_wandb_patched: bool            = False\n",
        "is_wandb_initialised: bool        = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ebtysClgOrVL"
      },
      "outputs": [],
      "source": [
        "def get_epochs() -> int:\n",
        "  global epoch\n",
        "  return epoch\n",
        "\n",
        "def set_epochs(\n",
        "  value: int\n",
        ") -> None:\n",
        "  global epoch\n",
        "  epoch = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mBxDaltYaJ_o"
      },
      "outputs": [],
      "source": [
        "def get_is_to_fork_model() -> bool:\n",
        "  global is_to_fork_model\n",
        "  return is_to_fork_model\n",
        "\n",
        "def set_is_to_fork_model(\n",
        "  value: bool\n",
        ") -> None:\n",
        "  global is_to_fork_model\n",
        "  is_to_fork_model = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jMy6CLL5-ES2"
      },
      "outputs": [],
      "source": [
        "def get_is_tensorboard_initialised() -> bool:\n",
        "  global is_tensorboard_initialised\n",
        "  return is_tensorboard_initialised\n",
        "\n",
        "def set_is_tensorboard_initialised(\n",
        "  value: bool\n",
        ") -> None:\n",
        "  global is_tensorboard_initialised\n",
        "  is_tensorboard_initialised = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cJvoWln--EnG"
      },
      "outputs": [],
      "source": [
        "def get_is_wandb_patched() -> bool:\n",
        "  global is_wandb_patched\n",
        "  return is_wandb_patched\n",
        "\n",
        "def set_is_wandb_patched(\n",
        "  value: bool\n",
        ") -> None:\n",
        "  global is_wandb_patched\n",
        "  is_wandb_patched = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "o68VTo-W-E8a"
      },
      "outputs": [],
      "source": [
        "def get_is_wandb_initialised() -> bool:\n",
        "  global is_wandb_initialised\n",
        "  return is_wandb_initialised\n",
        "\n",
        "def set_is_wandb_initialised(\n",
        "  value: bool\n",
        ") -> None:\n",
        "  global is_wandb_initialised\n",
        "  is_wandb_initialised = value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "zcahzZctH6j7"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "  not isdir(\n",
        "    move_to\n",
        "  )\n",
        "):\n",
        "  !wget {url_to_dataset} -O {dataset_compressed_destination}\n",
        "\n",
        "  if(\n",
        "    isfile(\n",
        "      dataset_compressed_destination\n",
        "    )\n",
        "  ):\n",
        "    !unzip -qq {dataset_compressed_destination} -d /tmp\n",
        "\n",
        "  !rm {dataset_compressed_destination}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vCvuvhUuziPC"
      },
      "outputs": [],
      "source": [
        "def get_label_layout() -> dict:\n",
        "  global labels_mapped\n",
        "  return labels_mapped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "buTam2YBiKKD"
      },
      "outputs": [],
      "source": [
        "def is_in_labels_mapped(\n",
        "    key: str\n",
        ") -> bool:\n",
        "  global labels_mapped\n",
        "  for label in labels_mapped:\n",
        "    if label == key:\n",
        "      return True\n",
        "\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7WdRmxXxiMym"
      },
      "outputs": [],
      "source": [
        "def parse_for_labels(\n",
        "  xml_file: str\n",
        ") -> list:\n",
        "  returnable: list = list()\n",
        "\n",
        "  tree = ET.parse(\n",
        "      xml_file\n",
        "  )\n",
        "\n",
        "  root = tree.getroot()\n",
        "\n",
        "  for object in root.iter(\n",
        "    get_label_object()\n",
        "  ):\n",
        "    class_name = find_class_name(\n",
        "      object\n",
        "    )\n",
        "\n",
        "    returnable.append(\n",
        "        class_name\n",
        "    )\n",
        "\n",
        "  return returnable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KBXMChhRXOXc"
      },
      "outputs": [],
      "source": [
        "if (\n",
        "  not isdir(\n",
        "    move_to\n",
        "  )\n",
        "):\n",
        "  final_destination: str = join(\n",
        "    destination_to_dataset,\n",
        "    'Dataset'\n",
        "  )\n",
        "\n",
        "  if (\n",
        "    isdir(\n",
        "      final_destination\n",
        "    )\n",
        "  ):\n",
        "    !mv {final_destination} {move_to}\n",
        "    !rm {destination_to_dataset} -R\n",
        "    !ls {move_to}\n",
        "  else:\n",
        "    !mv {destination_to_dataset} {move_to}\n",
        "\n",
        "locations_of_annotations: str = join(\n",
        "    move_to,\n",
        "    'Annotations'\n",
        ")\n",
        "\n",
        "for xml_file in listdir(\n",
        "    locations_of_annotations\n",
        "):\n",
        "  full_path_to_xml = join(\n",
        "      locations_of_annotations,\n",
        "      xml_file\n",
        "  )\n",
        "\n",
        "  labels = parse_for_labels(\n",
        "      full_path_to_xml\n",
        "  )\n",
        "\n",
        "  for label in labels:\n",
        "    normalised_label = label.lower()\n",
        "\n",
        "    if not is_in_labels_mapped(\n",
        "        normalised_label\n",
        "    ):\n",
        "      labels_mapped.append(\n",
        "          normalised_label\n",
        "      )\n",
        "\n",
        "labels_mapped.sort()\n",
        "\n",
        "size_of_labels: int = len(\n",
        "    labels_mapped\n",
        ")\n",
        "\n",
        "labels_mapped: dict = dict(\n",
        "    zip(\n",
        "        range(\n",
        "            size_of_labels\n",
        "        ),\n",
        "        labels_mapped\n",
        "    )\n",
        ")\n",
        "\n",
        "found_images = listdir(\n",
        "    join(\n",
        "        move_to,\n",
        "        'Images'\n",
        "    )\n",
        ")\n",
        "\n",
        "found_annotations = listdir(\n",
        "    join(\n",
        "        move_to,\n",
        "        'Annotations'\n",
        "    )\n",
        ")\n",
        "\n",
        "found_images.sort()\n",
        "found_annotations.sort()\n",
        "\n",
        "found: list = list()\n",
        "\n",
        "for idx in range(\n",
        "    len(\n",
        "        found_images\n",
        "    )\n",
        "):\n",
        "  found.append(\n",
        "      (\n",
        "          found_images[idx],\n",
        "          found_annotations[idx]\n",
        "      )\n",
        "  )\n",
        "\n",
        "del found_images, found_annotations\n",
        "\n",
        "size_of_found_files: int = len(\n",
        "  found\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "FO2JyW7Abswq"
      },
      "outputs": [],
      "source": [
        "def get_initial_learning_rate() -> float:\n",
        "  global hp_initial_learning_rate\n",
        "  return hp_initial_learning_rate\n",
        "\n",
        "def get_batch_size() -> int:\n",
        "  global batch_size\n",
        "  return batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5PzhCceqhwgr"
      },
      "outputs": [],
      "source": [
        "def get_target_configuration() -> dict:\n",
        "  global \\\n",
        "    target_input_width, \\\n",
        "    target_input_height, \\\n",
        "    target_input_channels\n",
        "\n",
        "  return {\n",
        "    'width': target_input_width,\n",
        "    'height': target_input_height,\n",
        "    'channels': target_input_channels\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "rhyqQjiBhhp5"
      },
      "outputs": [],
      "source": [
        "def get_dataset_configuration() -> dict:\n",
        "  global dataset_split_at, iterate_training\n",
        "\n",
        "  return {\n",
        "      'epochs': get_epochs(),\n",
        "      'iterations': iterate_training,\n",
        "      'batch_size': get_batch_size(),\n",
        "\n",
        "      'split': dataset_split_at,\n",
        "\n",
        "      'target': get_target_configuration()\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "HGAozK1jcB-Y"
      },
      "outputs": [],
      "source": [
        "def get_learning_configuration() -> dict:\n",
        "  return {\n",
        "      'initial': get_initial_learning_rate(),\n",
        "      'decay': {\n",
        "        'steps': hp_decay_steps,\n",
        "        'rate': hp_decay_rate\n",
        "      }\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "idrjDeC-cRKo"
      },
      "outputs": [],
      "source": [
        "def get_vision_configuration() -> list:\n",
        "  global                      \\\n",
        "    model_width,              \\\n",
        "    model_height,             \\\n",
        "    model_channels\n",
        "\n",
        "  return (\n",
        "      model_width,\n",
        "      model_height,\n",
        "      model_channels\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "HoqCl_SbX3HX"
      },
      "outputs": [],
      "source": [
        "model_project_version_build: str = 'build.alpha.YoloV8'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "GA5jTLQXiLGC"
      },
      "outputs": [],
      "source": [
        "def get_model_configuration() -> dict:\n",
        "  global model_project_version_build\n",
        "  \n",
        "  return {\n",
        "    'vision': get_vision_configuration(),\n",
        "    'learning': get_learning_configuration(),\n",
        "    'name': model_project_version_build,\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "t5HhSft6T-lt"
      },
      "outputs": [],
      "source": [
        "def configuration():\n",
        "  \n",
        "  return {\n",
        "    'model': get_model_configuration(),\n",
        "    'dataset': get_dataset_configuration(),\n",
        "\n",
        "    'formats': {\n",
        "        'bounding box': get_bounding_format()\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZYrK9HtEtp1"
      },
      "source": [
        "## Start of Script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "bR9vgHUIXjL-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 93169), started 1 day, 4:00:29 ago. (Use '!kill 93169' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-c4e9efcccbbaec82\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-c4e9efcccbbaec82\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if not isdir(\n",
        "    location_for_tensorboard\n",
        "):\n",
        "  mkdir(\n",
        "      location_for_tensorboard\n",
        "  )\n",
        "\n",
        "if not(\n",
        "  is_tensorboard_initialised\n",
        "):\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir {location_for_tensorboard}\n",
        "\n",
        "  is_tensorboard_initialised = True\n",
        "\n",
        "\n",
        "if(\n",
        "    (wandb_to_sync_tensorboard)\n",
        "    and\n",
        "    not(is_wandb_patched)\n",
        "):\n",
        "  wandb.tensorboard.patch(\n",
        "      root_logdir = location_for_tensorboard\n",
        "  )\n",
        "\n",
        "  is_wandb_patched = True\n",
        "\n",
        "if (\n",
        "  use_of_wandb\n",
        "  and\n",
        "  not(\n",
        "    is_wandb_initialised\n",
        "  )\n",
        "):\n",
        "  init(\n",
        "      name              = project_name,\n",
        "      project           = wandb_project_name,\n",
        "      entity            = wandb_entity_name,\n",
        "      save_code         = wandb_to_save_code,\n",
        "      config            = configuration(),\n",
        "      sync_tensorboard  = wandb_to_sync_tensorboard,\n",
        "      job_type          = wandb_job_type,\n",
        "      tags              = wandb_tags,\n",
        "      group             = wandb_group,\n",
        "      settings          = wandb.Settings(\n",
        "        start_method = 'thread'\n",
        "      )\n",
        "  )\n",
        "\n",
        "  is_wandb_initialised = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-28 19:43:42.714293: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-28 19:43:42.714351: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-28 19:43:42.714363: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-28 19:43:42.893996: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-28 19:43:42.894017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
            "2024-01-28 19:43:42.894043: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-28 19:43:42.894061: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
            "Your kernel may have been built without NUMA support.\n",
            "2024-01-28 19:43:42.894077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5950 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
            "/home/spectre/.local/lib/python3.10/site-packages/keras_cv/src/models/task.py:43: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/home/spectre/.local/lib/python3.10/site-packages/keras_cv/src/models/task.py:43: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        }
      ],
      "source": [
        "from keras.optimizers \\\n",
        "  import AdamW\n",
        "\n",
        "from keras.models import clone_model\n",
        "\n",
        "model_artifact = wandb.use_artifact(\n",
        "    'designermadsen/Spectre/keras:latest'\n",
        ")\n",
        "\n",
        "load_model_from = model_artifact.download()\n",
        "\n",
        "model = load_model(\n",
        "  join(\n",
        "    load_model_from, \n",
        "    'model.final.keras'\n",
        "  ),\n",
        "  compile = False\n",
        ")\n",
        "\n",
        "is_to_reset_model: bool = False\n",
        "\n",
        "if is_to_reset_model:\n",
        "  print('=== Cloning model. ===')\n",
        "  model = clone_model(\n",
        "    model\n",
        "  )\n",
        "\n",
        "if show_summary_of_model:\n",
        "  model.summary()\n",
        "\n",
        "learning_rate: ExponentialDecay = ExponentialDecay(\n",
        "  hp_initial_learning_rate,\n",
        "  decay_steps = hp_decay_steps,\n",
        "  decay_rate  = hp_decay_rate\n",
        ")\n",
        "\n",
        "optimizer = AdamW(\n",
        "    learning_rate   = hp_initial_learning_rate,\n",
        "    global_clipnorm = global_clip_at,\n",
        "    use_ema         = True,\n",
        "    amsgrad         = True\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    classification_loss = 'binary_crossentropy',\n",
        "    box_loss            = 'ciou',\n",
        "    optimizer           = optimizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxWm73k3XeyU"
      },
      "source": [
        "## Setup Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "JCv8C_5U2lRB"
      },
      "outputs": [],
      "source": [
        "def get_color_spectrum() -> tuple:\n",
        "  return (\n",
        "      zero(),\n",
        "      max_limit_for_rgb_color()\n",
        "  )\n",
        "\n",
        "def get_shuffle_sample_by() -> int:\n",
        "  global shuffle_sample_by\n",
        "  return shuffle_sample_by\n",
        "\n",
        "def maximum_for_int16() -> int:\n",
        "  return 65535\n",
        "\n",
        "def generate_random_integer() -> int:\n",
        "  begin: int  = one()\n",
        "  end: int    = maximum_for_int16()\n",
        "\n",
        "  return SystemRandom().randint(\n",
        "      begin,\n",
        "      end\n",
        "  )\n",
        "\n",
        "def is_in_formats(\n",
        "    value: str\n",
        ") -> bool:\n",
        "  global formats\n",
        "\n",
        "  for e in formats:\n",
        "    if value == e:\n",
        "      return True\n",
        "\n",
        "  return False\n",
        "\n",
        "def generate_grid_ratio() -> float:\n",
        "  return SystemRandom().uniform(\n",
        "      0.125,\n",
        "      0.999981\n",
        "  )\n",
        "\n",
        "def generate_grid_rotation() -> float:\n",
        "  return SystemRandom().uniform(\n",
        "    0.0,\n",
        "    0.9\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "n82y7r92x3Z0"
      },
      "outputs": [],
      "source": [
        "if not is_dataset_saved:\n",
        "  augmentation_layers: list = list()\n",
        "\n",
        "  if use_resize:\n",
        "    augmentation_layers.append(\n",
        "        cv_layers.Resizing(\n",
        "            height              = target_input_height,\n",
        "            width               = target_input_width,\n",
        "            pad_to_aspect_ratio = True,\n",
        "            bounding_box_format = get_bounding_format()\n",
        "        )\n",
        "    )\n",
        "\n",
        "  if use_jitter_resize:\n",
        "    augmentation_layers.append(\n",
        "      cv_layers.JitteredResize(\n",
        "        target_size = (\n",
        "          target_input_height,\n",
        "          target_input_width\n",
        "        ),\n",
        "        scale_factor = (\n",
        "          scale_boundary_lowest,\n",
        "          scale_boundary_highest\n",
        "        ),\n",
        "        bounding_box_format = get_bounding_format()\n",
        "      )\n",
        "    )\n",
        "\n",
        "  if use_random_brightness:\n",
        "    augmentation_layers.append(\n",
        "        cv_layers.RandomBrightness(\n",
        "            factor      = (-0.5, 0.5),\n",
        "            value_range = get_color_spectrum()\n",
        "        )\n",
        "    )\n",
        "\n",
        "  if use_random_flip:\n",
        "    augmentation_layers.append(\n",
        "        cv_layers.RandomFlip(\n",
        "            mode                = 'horizontal',\n",
        "            bounding_box_format = get_bounding_format()\n",
        "        )\n",
        "    )\n",
        "\n",
        "  if use_random_contrast:\n",
        "    augmentation_layers.append(\n",
        "        cv_layers.RandomContrast(\n",
        "            factor      = (-0.5, 0.5),\n",
        "            value_range = get_color_spectrum()\n",
        "        )\n",
        "    )\n",
        "\n",
        "  if use_grayscale_augmentation:\n",
        "    augmentation_layers.append(\n",
        "        cv_layers.Grayscale(\n",
        "            output_channels = get_image_channels()\n",
        "        )\n",
        "    )\n",
        "\n",
        "  if use_grid_mask:\n",
        "    augmentation_layers.append(\n",
        "        cv_layers.GridMask(\n",
        "            ratio_factor    = (\n",
        "              zero(),\n",
        "              generate_grid_ratio()\n",
        "            ),\n",
        "            rotation_factor = generate_grid_rotation(),\n",
        "            fill_mode       = 'constant',\n",
        "            fill_value      = zero()\n",
        "        )\n",
        "    )\n",
        "\n",
        "  resize_and_augmentations = keras.Sequential(\n",
        "      layers = augmentation_layers\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "HCFYekME0U2T"
      },
      "outputs": [],
      "source": [
        "dataset_paths: dict = dict(\n",
        "  {\n",
        "    label_dataset_images(): join(\n",
        "      move_to,\n",
        "      label_dataset_images().capitalize()\n",
        "    ),\n",
        "    label_dataset_annotations(): join(\n",
        "      move_to,\n",
        "      label_dataset_annotations().capitalize()\n",
        "    )\n",
        "  }\n",
        ")\n",
        "\n",
        "def get_dataset_paths() -> dict:\n",
        "  global dataset_paths\n",
        "  return dataset_paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "MrrjgxLh8ByJ"
      },
      "outputs": [],
      "source": [
        "def get_dataset_paths_entry(\n",
        "  key: str\n",
        ") -> str:\n",
        "  global dataset_paths\n",
        "  return get_dataset_paths()[key]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "dot: str = '.'\n",
        "\n",
        "def get_dot() -> str: \n",
        "    global dot\n",
        "    return dot\n",
        "\n",
        "\n",
        "jpg: str = str(\n",
        "    get_dot() \n",
        "    + \n",
        "    'jpg'\n",
        ").lower()\n",
        "\n",
        "jpeg: str = str(\n",
        "    get_dot() \n",
        "    + \n",
        "    'jpeg'\n",
        ").lower()\n",
        "\n",
        "xml: str = str(\n",
        "    get_dot()\n",
        "    +\n",
        "    'xml'\n",
        ")\n",
        "\n",
        "def get_jpg_label() -> str:\n",
        "    global jpg\n",
        "    return jpg \n",
        "\n",
        "def get_jpeg_label() -> str:\n",
        "    global jpeg\n",
        "    return jpeg\n",
        "\n",
        "def get_xml_label() -> str:\n",
        "    global xml\n",
        "    return xml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "tADyuQw8yrBD"
      },
      "outputs": [],
      "source": [
        "def file_has_jpeg_extension(\n",
        "      file_name: str\n",
        ") -> bool:\n",
        "  global dot\n",
        "  file_name_normalised = file_name.lower()\n",
        "\n",
        "  return (\n",
        "      file_name_normalised.endswith(\n",
        "            get_jpg_label()\n",
        "      )\n",
        "      or\n",
        "      file_name_normalised.endswith(\n",
        "            get_jpeg_label()\n",
        "      )\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "eqdGDYFDOrVd"
      },
      "outputs": [],
      "source": [
        "def file_has_xml_extension(\n",
        "      file_name: str\n",
        ") -> bool:\n",
        "  file_name_normalised = file_name.lower()\n",
        "\n",
        "  return file_name_normalised.endswith(\n",
        "        get_xml_label()\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "8bgfFxjnWzRF"
      },
      "outputs": [],
      "source": [
        "def find_filename(\n",
        "  root\n",
        ") -> str:\n",
        "  return str(\n",
        "    root.find(\n",
        "      'filename'\n",
        "    ).text\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "iY1Re3kjZt4I"
      },
      "outputs": [],
      "source": [
        "def find_x_minimum_boundary(\n",
        "  bounderies\n",
        ") -> float:\n",
        "  return float(\n",
        "    bounderies.find(\n",
        "      get_label_x_minimum()\n",
        "    ).text\n",
        "  )\n",
        "\n",
        "def find_y_minimum_boundary(\n",
        "  bounderies\n",
        ") -> float:\n",
        "  return float(\n",
        "    bounderies.find(\n",
        "      get_label_y_minimum()\n",
        "    ).text\n",
        "  )\n",
        "\n",
        "def find_x_maximum_boundary(\n",
        "  bounderies\n",
        ") -> float:\n",
        "  return float(\n",
        "    bounderies.find(\n",
        "      get_label_x_maximum()\n",
        "    ).text\n",
        "  )\n",
        "\n",
        "def find_y_maximum_boundary(\n",
        "  bounderies\n",
        ") -> float:\n",
        "  return float(\n",
        "    bounderies.find(\n",
        "      get_label_y_maximum()\n",
        "    ).text\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "P0sGyFGuOrVh"
      },
      "outputs": [],
      "source": [
        "def find_bounding_boxes(\n",
        "  bounderies\n",
        "):\n",
        "  xmin: float = find_x_minimum_boundary(\n",
        "    bounderies\n",
        "  )\n",
        "\n",
        "  ymin: float = find_y_minimum_boundary(\n",
        "    bounderies\n",
        "  )\n",
        "\n",
        "  xmax: float = find_x_maximum_boundary(\n",
        "    bounderies\n",
        "  )\n",
        "\n",
        "  ymax: float = find_y_maximum_boundary(\n",
        "    bounderies\n",
        "  )\n",
        "\n",
        "  return [\n",
        "    xmin,\n",
        "    ymin,\n",
        "    xmax,\n",
        "    ymax\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "r8Dhf0NOY5LA"
      },
      "outputs": [],
      "source": [
        "def find_bounderies(\n",
        "  object\n",
        "):\n",
        "  bounderies = object.find(\n",
        "    get_label_bounding_box()\n",
        "  )\n",
        "\n",
        "  return find_bounding_boxes(\n",
        "    bounderies\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "x6sDVYVlzBFj"
      },
      "outputs": [],
      "source": [
        "def parse_xml(\n",
        "    xml_file\n",
        "  ):\n",
        "    global            \\\n",
        "      dataset_paths,  \\\n",
        "      labels_mapped\n",
        "\n",
        "    tree = ET.parse(\n",
        "      xml_file\n",
        "    )\n",
        "\n",
        "    root = tree.getroot()\n",
        "\n",
        "    image_path = join(\n",
        "      get_dataset_paths()[\n",
        "        label_dataset_images()\n",
        "      ],\n",
        "      find_filename(\n",
        "        root\n",
        "      )\n",
        "    )\n",
        "\n",
        "    boxes: list = []\n",
        "    classes: list = []\n",
        "\n",
        "    for object in root.iter(\n",
        "      get_label_object()\n",
        "    ):\n",
        "      classes.append(\n",
        "        find_class_name(\n",
        "          object\n",
        "        )\n",
        "      )\n",
        "\n",
        "      boxes.append(\n",
        "          find_bounderies(\n",
        "            object\n",
        "          )\n",
        "      )\n",
        "\n",
        "    label_ids: list = [\n",
        "      list(\n",
        "        labels_mapped.keys()\n",
        "      )[\n",
        "        list(\n",
        "          labels_mapped.values()\n",
        "        ).index(\n",
        "          cls\n",
        "        )\n",
        "      ]\n",
        "        for cls in classes\n",
        "    ]\n",
        "\n",
        "    return (\n",
        "      image_path,\n",
        "      boxes,\n",
        "      label_ids\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "X07Z-CCbcIs8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45de3b000643478c8fc2c21d3db75034",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if not is_dataset_saved:\n",
        "  prefetch_buffer_size: int = get_tune_value()\n",
        "\n",
        "  xml_files = sorted(\n",
        "      [\n",
        "          join(\n",
        "              dataset_paths[\n",
        "                  label_dataset_annotations()\n",
        "              ],\n",
        "              file_name\n",
        "          )\n",
        "\n",
        "          for file_name in listdir(\n",
        "              dataset_paths[\n",
        "                  label_dataset_annotations()\n",
        "              ]\n",
        "          )\n",
        "\n",
        "          if file_has_xml_extension(\n",
        "              file_name\n",
        "          )\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  size_of_annotations = len(\n",
        "      xml_files\n",
        "  )\n",
        "\n",
        "  image_files = sorted(\n",
        "      [\n",
        "          join(\n",
        "              dataset_paths[\n",
        "                  label_dataset_images()\n",
        "              ],\n",
        "              file_name\n",
        "          )\n",
        "\n",
        "          for file_name in listdir(\n",
        "              dataset_paths[\n",
        "                  label_dataset_images()\n",
        "              ]\n",
        "          )\n",
        "\n",
        "          if file_has_jpeg_extension(\n",
        "              file_name\n",
        "          )\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  image_paths: list = []\n",
        "  boundaries: list  = []\n",
        "  classes: list     = []\n",
        "\n",
        "  for file \\\n",
        "    in tqdm(\n",
        "      xml_files\n",
        "  ):\n",
        "      image_path, boxes, label_ids = parse_xml(\n",
        "          file\n",
        "      )\n",
        "\n",
        "      image_paths.append(\n",
        "          image_path\n",
        "      )\n",
        "\n",
        "      boundaries.append(\n",
        "          boxes\n",
        "      )\n",
        "\n",
        "      classes.append(\n",
        "          label_ids\n",
        "      )\n",
        "\n",
        "  image_paths = constant(\n",
        "      image_paths\n",
        "  )\n",
        "\n",
        "  classes = constant(\n",
        "      classes\n",
        "  )\n",
        "\n",
        "  boundaries = constant(\n",
        "      boundaries\n",
        "  )\n",
        "\n",
        "  dataset = tensorflow.data.Dataset.from_tensor_slices(\n",
        "    (\n",
        "      image_paths,\n",
        "      classes,\n",
        "      boundaries\n",
        "    )\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "1luP5HMrOrVi"
      },
      "outputs": [],
      "source": [
        "def load_image(\n",
        "  image_path: str\n",
        "):\n",
        "  image = tensorflow.io.read_file(\n",
        "    image_path\n",
        "  )\n",
        "\n",
        "  image = tensorflow.image.decode_jpeg(\n",
        "    image,\n",
        "    channels = get_image_channels()\n",
        "  )\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "40nh-WavOrVi"
      },
      "outputs": [],
      "source": [
        "def load_dataset(\n",
        "  image_path,\n",
        "  classes,\n",
        "  bounderies\n",
        "):\n",
        "  image = load_image(\n",
        "    image_path\n",
        "  )\n",
        "\n",
        "  bounding_boxes = {\n",
        "    'classes': tensorflow.cast(\n",
        "      classes,\n",
        "      dtype=tensorflow.float32\n",
        "    ),\n",
        "    'boxes': bounderies\n",
        "  }\n",
        "\n",
        "  return {\n",
        "    label_dataset_images(): tensorflow.cast(\n",
        "      image,\n",
        "      dtype=tensorflow.float32\n",
        "    ),\n",
        "    'bounding_boxes': bounding_boxes\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "QqJLfNRGOrVi"
      },
      "outputs": [],
      "source": [
        "if not is_dataset_saved:\n",
        "  number_of_validation: int = int(\n",
        "      size_of_annotations\n",
        "      *\n",
        "      dataset_split_at\n",
        "  )\n",
        "\n",
        "  dataset = dataset.shuffle(\n",
        "      get_shuffle_sample_by()\n",
        "  )\n",
        "\n",
        "  validation_data = dataset.take(\n",
        "      number_of_validation\n",
        "  )\n",
        "\n",
        "  training_data = dataset.skip(\n",
        "      number_of_validation\n",
        "  )\n",
        "\n",
        "  training_data = training_data.map(\n",
        "      load_dataset,\n",
        "      num_parallel_calls = prefetch_buffer_size\n",
        "  )\n",
        "\n",
        "  training_data = training_data.shuffle(\n",
        "      get_shuffle_sample_by()\n",
        "  )\n",
        "\n",
        "  training_data = training_data.ragged_batch(\n",
        "      batch_size,\n",
        "      drop_remainder = True\n",
        "  )\n",
        "\n",
        "  validation_data = validation_data.map(\n",
        "      load_dataset,\n",
        "      num_parallel_calls = prefetch_buffer_size\n",
        "  )\n",
        "\n",
        "  validation_data = validation_data.shuffle(\n",
        "      get_shuffle_sample_by()\n",
        "  )\n",
        "\n",
        "  validation_data = validation_data.ragged_batch(\n",
        "      batch_size,\n",
        "      drop_remainder = True\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "KXdkzJxi3z9h"
      },
      "outputs": [],
      "source": [
        "def dict_to_tuple(\n",
        "      inputs: dict\n",
        "  ) -> tuple:\n",
        "    bounding_boxes = bounding_box.to_dense(\n",
        "      inputs['bounding_boxes'],\n",
        "      max_boxes = 32\n",
        "    )\n",
        "\n",
        "    return (\n",
        "      inputs[\n",
        "        label_dataset_images()\n",
        "      ],\n",
        "      bounding_boxes\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "j3PrxTN73niR"
      },
      "outputs": [],
      "source": [
        "if not is_dataset_saved:\n",
        "  training_data = training_data.map(\n",
        "      resize_and_augmentations,\n",
        "      num_parallel_calls = prefetch_buffer_size\n",
        "  )\n",
        "\n",
        "  validation_data = validation_data.map(\n",
        "      resize_and_augmentations,\n",
        "      num_parallel_calls = prefetch_buffer_size\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "dfn-aWcGrPz0"
      },
      "outputs": [],
      "source": [
        "def flag_dataset_is_save() -> None:\n",
        "  global is_dataset_saved\n",
        "  if not is_dataset_saved:\n",
        "    is_dataset_saved = True\n",
        "\n",
        "\n",
        "def get_autotune():\n",
        "  from tensorflow.data import AUTOTUNE\n",
        "  return AUTOTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "3p9dv5DK39pH"
      },
      "outputs": [],
      "source": [
        "if not is_dataset_saved:\n",
        "  training_data = training_data.map(\n",
        "      dict_to_tuple,\n",
        "      num_parallel_calls = prefetch_buffer_size\n",
        "  )\n",
        "\n",
        "  validation_data = validation_data.map(\n",
        "      dict_to_tuple,\n",
        "      num_parallel_calls = prefetch_buffer_size\n",
        "  )\n",
        "\n",
        "  flag_dataset_is_save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmfHrdgjRoCZ"
      },
      "source": [
        "## Callbacks and training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "wOvRr-YNlAaV"
      },
      "outputs": [],
      "source": [
        "def save_best_model():\n",
        "  global callbacks\n",
        "\n",
        "  for c in callbacks:\n",
        "    if isinstance(\n",
        "        c,\n",
        "        EvaluateCOCOMetricsByCallback\n",
        "    ):\n",
        "      pass\n",
        "\n",
        "class EvaluateCOCOMetricsByCallback(\n",
        "    Callback\n",
        "):\n",
        "  def __init__(\n",
        "      self,\n",
        "      data,\n",
        "      bounding_box: str,\n",
        "      evaluation_frequency: int = int(\n",
        "          1e9\n",
        "      )\n",
        "  ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.data = data.prefetch(\n",
        "        buffer_size = get_autotune()\n",
        "    )\n",
        "\n",
        "    self.bounding_box = bounding_box\n",
        "    \n",
        "    self.metrics = BoxCOCOMetrics(\n",
        "        bounding_box_format = self.bounding_box,\n",
        "        evaluate_freq = evaluation_frequency\n",
        "    )\n",
        "\n",
        "    self.save_directory: str = '/tmp/model.weights'\n",
        "    self.save_format: str = 'keras'\n",
        "\n",
        "    self.best_map = -1.0\n",
        "\n",
        "    self.wandb: dict = {\n",
        "        'best': None\n",
        "    }\n",
        "\n",
        "    self.save_best_version_of_model_to_wandb: bool = False\n",
        "\n",
        "  def is_to_save_best_version_of_model_to_wandb(\n",
        "    self\n",
        "  ) -> bool:\n",
        "    return self.save_best_version_of_model_to_wandb\n",
        "\n",
        "  def get_best_map(\n",
        "    self\n",
        "  ):\n",
        "    return self.best_map\n",
        "\n",
        "  def set_best_map(\n",
        "    self,\n",
        "    value\n",
        "  ) -> None:\n",
        "    self.best_map = value\n",
        "\n",
        "  def is_best_map(\n",
        "    self,\n",
        "    evaluate_by\n",
        "  ):\n",
        "    return (\n",
        "        evaluate_by\n",
        "        >=\n",
        "        self.get_best_map()\n",
        "    )\n",
        "\n",
        "  def __save_bm_tp(\n",
        "    self\n",
        "  ) -> None:\n",
        "    pass\n",
        "\n",
        "  def trigger_save_model(\n",
        "    self\n",
        "  ) -> None:\n",
        "    self.model.save_weights(\n",
        "        self.save_directory,\n",
        "        overwrite = True,\n",
        "        save_format = self.save_format\n",
        "    )\n",
        "\n",
        "    self.__save_bm_tp()\n",
        "\n",
        "  def on_epoch_end(\n",
        "      self,\n",
        "      epoch,\n",
        "      logs\n",
        "  ):\n",
        "    self.metrics.reset_state()\n",
        "\n",
        "    for batch in self.data:\n",
        "      images, y_true = (\n",
        "          batch[\n",
        "              zero()\n",
        "          ],\n",
        "          batch[\n",
        "              one()\n",
        "          ]\n",
        "      )\n",
        "\n",
        "      y_pred = self.model.predict(\n",
        "          images,\n",
        "          verbose = zero()\n",
        "      )\n",
        "\n",
        "      self.metrics.update_state(\n",
        "          y_true,\n",
        "          y_pred\n",
        "      )\n",
        "\n",
        "    metrics = self.metrics.result(\n",
        "        force = True\n",
        "    )\n",
        "\n",
        "    logs.update(\n",
        "        metrics\n",
        "    )\n",
        "\n",
        "    current_map = metrics[\n",
        "        label_map()\n",
        "    ]\n",
        "\n",
        "    if self.is_best_map(\n",
        "        current_map\n",
        "    ):\n",
        "      self.set_best_map(\n",
        "          current_map\n",
        "      )\n",
        "      self.trigger_save_model()\n",
        "\n",
        "    return logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkEhYvn40q88"
      },
      "source": [
        "## Setup of training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "KMWY4H3My2TT"
      },
      "outputs": [],
      "source": [
        "treat_fit_as_seperate: bool = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "vcnxAhrzy383"
      },
      "outputs": [],
      "source": [
        "def get_tensorboard_location():\n",
        "  global                      \\\n",
        "    location_for_tensorboard, \\\n",
        "    treat_fit_as_seperate\n",
        "\n",
        "  from time \\\n",
        "    import time_ns\n",
        "\n",
        "  if not treat_fit_as_seperate:\n",
        "    return location_for_tensorboard\n",
        "  else:\n",
        "    return join(\n",
        "        location_for_tensorboard,\n",
        "        str(\n",
        "            time_ns()\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "PoDgi80fXAf0"
      },
      "outputs": [],
      "source": [
        "def generate_tensorboard_callback():\n",
        "  callback = TensorBoard(\n",
        "        log_dir                 = get_tensorboard_location(),\n",
        "        histogram_freq          = one(),\n",
        "        write_graph             = True,\n",
        "        write_images            = True,\n",
        "        write_steps_per_second  = True,\n",
        "        update_freq             = 'epoch',\n",
        "        embeddings_freq         = one()\n",
        "\n",
        "  )\n",
        "\n",
        "  return callback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "JDKMFG63TC7B"
      },
      "outputs": [],
      "source": [
        "def refresh_tensorboard_callback():\n",
        "  global callbacks\n",
        "\n",
        "  size_of_callbacks = len(\n",
        "    callbacks\n",
        "  )\n",
        "\n",
        "  for idx in range(\n",
        "    size_of_callbacks\n",
        "  ):\n",
        "    callback = callbacks[idx]\n",
        "\n",
        "    if isinstance(\n",
        "        callback,\n",
        "        TensorBoard\n",
        "    ):\n",
        "      callbacks[idx] = generate_tensorboard_callback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "8fBrVklc8W3m"
      },
      "outputs": [],
      "source": [
        "histories: list = list()\n",
        "\n",
        "def get_histories() -> list:\n",
        "  global histories\n",
        "  return histories\n",
        "\n",
        "def set_histories(\n",
        "  value: list\n",
        ") -> None:\n",
        "  global histories\n",
        "  histories = value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "iVJz0KSbyvg6"
      },
      "outputs": [],
      "source": [
        "def append_history(\n",
        "  value\n",
        ") -> None:\n",
        "  global histories\n",
        "\n",
        "  histories.append(\n",
        "    value\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "1z26wGeZv_wo"
      },
      "outputs": [],
      "source": [
        "def generate_callbacks():\n",
        "  callbacks: list = list()\n",
        "\n",
        "  callbacks.append(\n",
        "    WandbMetricsLogger()\n",
        "  )\n",
        "\n",
        "  callbacks.append(\n",
        "      generate_tensorboard_callback()\n",
        "  )\n",
        "\n",
        "  if use_coco_metrics:\n",
        "    callbacks.append(\n",
        "        EvaluateCOCOMetricsByCallback(\n",
        "            validation_data,\n",
        "            get_bounding_format()\n",
        "        )\n",
        "    )\n",
        "\n",
        "  return callbacks\n",
        "\n",
        "callbacks = generate_callbacks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "q_2VzrD3hk0K"
      },
      "outputs": [],
      "source": [
        "def refresh_dataset():\n",
        "  global              \\\n",
        "    batch_size,       \\\n",
        "    training_data,    \\\n",
        "    validation_data,  \\\n",
        "    shuffle_sample_by\n",
        "\n",
        "  training_data = training_data.shuffle(\n",
        "    shuffle_sample_by\n",
        "  )\n",
        "\n",
        "  validation_data = validation_data.shuffle(\n",
        "    shuffle_sample_by\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "j6KTmuKPyrZM"
      },
      "outputs": [],
      "source": [
        "def flag_evaluation_has_run():\n",
        "  global eval_has_run\n",
        "\n",
        "  if not eval_has_run:\n",
        "    eval_has_run = True\n",
        "\n",
        "def get_evaluation_batch_size() -> int:\n",
        "  return int(\n",
        "    get_batch_size()\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "aJ272yFJjoqt"
      },
      "outputs": [],
      "source": [
        "def range_for_histories_size():\n",
        "  global histories\n",
        "\n",
        "  return range(\n",
        "      len(\n",
        "          histories\n",
        "      )\n",
        "  )\n",
        "\n",
        "def append_history_to_log(\n",
        "  history\n",
        "):\n",
        "  global histories\n",
        "\n",
        "  histories.append(\n",
        "      history.history\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "JJkZQfIxyCxH"
      },
      "outputs": [],
      "source": [
        "def retrieve_location_for_best_model():\n",
        "  global callbacks\n",
        "\n",
        "  location_of_best_model: None | str = None\n",
        "\n",
        "  for callback in callbacks:\n",
        "    if isinstance(\n",
        "        callback,\n",
        "        EvaluateCOCOMetricsByCallback\n",
        "    ):\n",
        "      location_of_best_model: str = callback.save_directory\n",
        "\n",
        "  return location_of_best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "nD02tYZUyqhi"
      },
      "outputs": [],
      "source": [
        "def set_model(\n",
        "  value: Model\n",
        ") -> None:\n",
        "  global model\n",
        "  model = value\n",
        "  \n",
        "def get_model() -> Model:\n",
        "  global model\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "b4Xq7UIeylMs"
      },
      "outputs": [],
      "source": [
        "def save_model() -> None:\n",
        "  global load_model_from\n",
        "\n",
        "  get_model().save(\n",
        "      load_model_from,\n",
        "      save_format = 'keras'\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "2He8nWNGuaTI"
      },
      "outputs": [],
      "source": [
        "def training_of_model(\n",
        "  use_of_multiprocessing: bool,\n",
        "  use_of_workers: int\n",
        ") -> None:\n",
        "  global                          \\\n",
        "    training_data,                \\\n",
        "    validation_data,              \\\n",
        "    callbacks,                    \\\n",
        "    tensorflow_verbosity,         \\\n",
        "    subsample_from_dataset\n",
        "\n",
        "  history = get_model().fit(\n",
        "    training_data.prefetch(\n",
        "        buffer_size = get_autotune()\n",
        "    ),\n",
        "    validation_data         = validation_data.prefetch(\n",
        "        buffer_size = get_autotune()\n",
        "    ),\n",
        "    epochs                  = get_epochs(),\n",
        "    callbacks               = callbacks,\n",
        "    workers                 = use_of_workers,\n",
        "    use_multiprocessing     = use_of_multiprocessing,\n",
        "    verbose                 = tensorflow_verbosity\n",
        "  )\n",
        "\n",
        "  append_history_to_log(\n",
        "      history\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "4fngE1q-a7mY"
      },
      "outputs": [],
      "source": [
        "def load_best_model() -> None:\n",
        "  get_model().load_weights(\n",
        "    retrieve_location_for_best_model()\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "j2F7nSs3bkO6"
      },
      "outputs": [],
      "source": [
        "def training_process():\n",
        "  global                    \\\n",
        "    iterate_training,       \\\n",
        "    use_of_multiprocessing, \\\n",
        "    use_of_workers,         \\\n",
        "    callbacks\n",
        "  \n",
        "  callbacks = generate_callbacks()\n",
        "\n",
        "  for i in range(\n",
        "    one(),\n",
        "    int(\n",
        "      iterate_training \n",
        "      + \n",
        "      one()\n",
        "    )\n",
        "  ):\n",
        "    training_of_model(\n",
        "        use_of_multiprocessing,\n",
        "        use_of_workers\n",
        "    )\n",
        "\n",
        "    load_best_model()\n",
        "    refresh_tensorboard_callback()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "xqPDd4_i-NDU"
      },
      "outputs": [],
      "source": [
        "def is_to_stop_when_done() -> None:\n",
        "  global stop_when_done\n",
        "\n",
        "  if stop_when_done:\n",
        "    raise Exception(\n",
        "        'Done'\n",
        "    )\n",
        "\n",
        "def training() -> None:\n",
        "  training_process()\n",
        "  is_to_stop_when_done()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-28 19:48:50.733076: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
            "2024-01-28 19:48:51.013160: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-01-28 19:48:51.041811: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-28 19:48:51.041870: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-28 19:48:51.042629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-28 19:48:51.048086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-28 19:48:51.668876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-01-28 19:49:11.824840: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
            "2024-01-28 19:49:11.918837: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
            "2024-01-28 19:49:28.007863: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.83GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2024-01-28 19:49:28.056510: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.83GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2024-01-28 19:49:30.671440: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2024-01-28 19:49:30.671488: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2024-01-28 19:49:30.728750: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2024-01-28 19:49:30.728815: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.23GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
            "2024-01-28 19:49:34.193992: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fa08ff75470 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-01-28 19:49:34.194034: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
            "2024-01-28 19:49:34.199268: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1706467774.303070   19452 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "187/187 [==============================] - ETA: 0s - loss: 0.7771 - box_loss: 0.7761 - class_loss: 9.3970e-04"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/spectre/.local/lib/python3.10/site-packages/keras_cv/src/models/backbones/backbone.py:44: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
            "/home/spectre/.local/lib/python3.10/site-packages/keras_cv/src/models/backbones/backbone.py:44: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "187/187 [==============================] - 143s 468ms/step - loss: 0.7771 - box_loss: 0.7761 - class_loss: 9.3970e-04 - val_loss: 1.2458 - val_box_loss: 1.2438 - val_class_loss: 0.0020 - MaP: 0.5017 - MaP@[IoU=50]: 0.7032 - MaP@[IoU=75]: 0.5690 - MaP@[area=small]: 0.3947 - MaP@[area=medium]: 0.4544 - MaP@[area=large]: 0.6803 - Recall@[max_detections=1]: 0.3849 - Recall@[max_detections=10]: 0.5270 - Recall@[max_detections=100]: 0.5270 - Recall@[area=small]: 0.4060 - Recall@[area=medium]: 0.4796 - Recall@[area=large]: 0.7034\n",
            "Epoch 2/11\n",
            "187/187 [==============================] - 80s 426ms/step - loss: 0.7401 - box_loss: 0.7392 - class_loss: 9.4102e-04 - val_loss: 1.2272 - val_box_loss: 1.2245 - val_class_loss: 0.0028 - MaP: 0.5320 - MaP@[IoU=50]: 0.7312 - MaP@[IoU=75]: 0.6225 - MaP@[area=small]: 0.3954 - MaP@[area=medium]: 0.4570 - MaP@[area=large]: 0.6552 - Recall@[max_detections=1]: 0.4344 - Recall@[max_detections=10]: 0.5625 - Recall@[max_detections=100]: 0.5625 - Recall@[area=small]: 0.4072 - Recall@[area=medium]: 0.4894 - Recall@[area=large]: 0.6766\n",
            "Epoch 3/11\n",
            "187/187 [==============================] - 79s 421ms/step - loss: 0.7403 - box_loss: 0.7394 - class_loss: 9.3801e-04 - val_loss: 1.2671 - val_box_loss: 1.2644 - val_class_loss: 0.0027 - MaP: 0.5121 - MaP@[IoU=50]: 0.7152 - MaP@[IoU=75]: 0.5857 - MaP@[area=small]: 0.3825 - MaP@[area=medium]: 0.4786 - MaP@[area=large]: 0.6175 - Recall@[max_detections=1]: 0.4258 - Recall@[max_detections=10]: 0.5465 - Recall@[max_detections=100]: 0.5465 - Recall@[area=small]: 0.3932 - Recall@[area=medium]: 0.5064 - Recall@[area=large]: 0.6446\n",
            "Epoch 4/11\n",
            "187/187 [==============================] - 79s 423ms/step - loss: 0.7456 - box_loss: 0.7446 - class_loss: 9.4229e-04 - val_loss: 1.2486 - val_box_loss: 1.2465 - val_class_loss: 0.0021 - MaP: 0.4700 - MaP@[IoU=50]: 0.6708 - MaP@[IoU=75]: 0.5315 - MaP@[area=small]: 0.3505 - MaP@[area=medium]: 0.3838 - MaP@[area=large]: 0.6173 - Recall@[max_detections=1]: 0.3895 - Recall@[max_detections=10]: 0.5064 - Recall@[max_detections=100]: 0.5064 - Recall@[area=small]: 0.3632 - Recall@[area=medium]: 0.4211 - Recall@[area=large]: 0.6461\n",
            "Epoch 5/11\n",
            "187/187 [==============================] - 80s 426ms/step - loss: 0.7369 - box_loss: 0.7360 - class_loss: 9.2960e-04 - val_loss: 1.2042 - val_box_loss: 1.2024 - val_class_loss: 0.0018 - MaP: 0.5538 - MaP@[IoU=50]: 0.8015 - MaP@[IoU=75]: 0.6205 - MaP@[area=small]: 0.3580 - MaP@[area=medium]: 0.5233 - MaP@[area=large]: 0.6680 - Recall@[max_detections=1]: 0.4712 - Recall@[max_detections=10]: 0.6027 - Recall@[max_detections=100]: 0.6027 - Recall@[area=small]: 0.3695 - Recall@[area=medium]: 0.5654 - Recall@[area=large]: 0.6975\n",
            "Epoch 6/11\n",
            "187/187 [==============================] - 80s 426ms/step - loss: 0.7161 - box_loss: 0.7151 - class_loss: 9.1825e-04 - val_loss: 1.2593 - val_box_loss: 1.2574 - val_class_loss: 0.0019 - MaP: 0.5173 - MaP@[IoU=50]: 0.7301 - MaP@[IoU=75]: 0.5997 - MaP@[area=small]: 0.3746 - MaP@[area=medium]: 0.5207 - MaP@[area=large]: 0.6176 - Recall@[max_detections=1]: 0.4325 - Recall@[max_detections=10]: 0.5529 - Recall@[max_detections=100]: 0.5529 - Recall@[area=small]: 0.3848 - Recall@[area=medium]: 0.5472 - Recall@[area=large]: 0.6490\n",
            "Epoch 7/11\n",
            "187/187 [==============================] - 81s 435ms/step - loss: 0.7301 - box_loss: 0.7291 - class_loss: 9.3692e-04 - val_loss: 1.2641 - val_box_loss: 1.2621 - val_class_loss: 0.0020 - MaP: 0.5387 - MaP@[IoU=50]: 0.7482 - MaP@[IoU=75]: 0.6639 - MaP@[area=small]: 0.3730 - MaP@[area=medium]: 0.4962 - MaP@[area=large]: 0.6472 - Recall@[max_detections=1]: 0.4489 - Recall@[max_detections=10]: 0.5694 - Recall@[max_detections=100]: 0.5694 - Recall@[area=small]: 0.3841 - Recall@[area=medium]: 0.5235 - Recall@[area=large]: 0.6682\n",
            "Epoch 8/11\n",
            "187/187 [==============================] - 80s 429ms/step - loss: 0.7298 - box_loss: 0.7289 - class_loss: 9.2430e-04 - val_loss: 1.2495 - val_box_loss: 1.2469 - val_class_loss: 0.0026 - MaP: 0.4924 - MaP@[IoU=50]: 0.6982 - MaP@[IoU=75]: 0.5790 - MaP@[area=small]: 0.3579 - MaP@[area=medium]: 0.4638 - MaP@[area=large]: 0.5960 - Recall@[max_detections=1]: 0.4088 - Recall@[max_detections=10]: 0.5288 - Recall@[max_detections=100]: 0.5288 - Recall@[area=small]: 0.3681 - Recall@[area=medium]: 0.4981 - Recall@[area=large]: 0.6255\n",
            "Epoch 9/11\n",
            "187/187 [==============================] - 80s 426ms/step - loss: 0.7113 - box_loss: 0.7104 - class_loss: 9.1306e-04 - val_loss: 1.2502 - val_box_loss: 1.2481 - val_class_loss: 0.0021 - MaP: 0.5447 - MaP@[IoU=50]: 0.7748 - MaP@[IoU=75]: 0.6351 - MaP@[area=small]: 0.3703 - MaP@[area=medium]: 0.5040 - MaP@[area=large]: 0.6335 - Recall@[max_detections=1]: 0.4728 - Recall@[max_detections=10]: 0.5907 - Recall@[max_detections=100]: 0.5907 - Recall@[area=small]: 0.3790 - Recall@[area=medium]: 0.5442 - Recall@[area=large]: 0.6604\n",
            "Epoch 10/11\n",
            "187/187 [==============================] - 82s 437ms/step - loss: 0.7318 - box_loss: 0.7309 - class_loss: 9.1234e-04 - val_loss: 1.2265 - val_box_loss: 1.2246 - val_class_loss: 0.0019 - MaP: 0.5032 - MaP@[IoU=50]: 0.7230 - MaP@[IoU=75]: 0.5873 - MaP@[area=small]: 0.3701 - MaP@[area=medium]: 0.4075 - MaP@[area=large]: 0.6487 - Recall@[max_detections=1]: 0.4184 - Recall@[max_detections=10]: 0.5365 - Recall@[max_detections=100]: 0.5365 - Recall@[area=small]: 0.3830 - Recall@[area=medium]: 0.4481 - Recall@[area=large]: 0.6704\n",
            "Epoch 11/11\n",
            "187/187 [==============================] - 80s 427ms/step - loss: 0.7276 - box_loss: 0.7266 - class_loss: 9.4128e-04 - val_loss: 1.2648 - val_box_loss: 1.2626 - val_class_loss: 0.0022 - MaP: 0.4774 - MaP@[IoU=50]: 0.6729 - MaP@[IoU=75]: 0.5765 - MaP@[area=small]: 0.3598 - MaP@[area=medium]: 0.4305 - MaP@[area=large]: 0.6310 - Recall@[max_detections=1]: 0.4008 - Recall@[max_detections=10]: 0.5117 - Recall@[max_detections=100]: 0.5117 - Recall@[area=small]: 0.3821 - Recall@[area=medium]: 0.4529 - Recall@[area=large]: 0.6645\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-28 20:04:35.771432: W tensorflow/core/util/tensor_slice_reader.cc:98] Could not open /tmp/model.weights: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n"
          ]
        }
      ],
      "source": [
        "training()\n",
        "\n",
        "wandb.log(\n",
        "    {\n",
        "        'histories': histories\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "2D7JwGlAiaqG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-01-28 20:04:36.282902: W tensorflow/core/util/tensor_slice_reader.cc:98] Could not open /tmp/model.weights: DATA_LOSS: not an sstable (bad magic number): perhaps your file is in a different file format and you need to use a different restore operator?\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Artifact keras>"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "load_best_model()\n",
        "\n",
        "model.save(\n",
        "    save_final_model_at,\n",
        "    overwrite   = True,\n",
        "    save_format = 'keras'\n",
        ")\n",
        "\n",
        "model_artifact = wandb.Artifact(\n",
        "    name    = 'keras',\n",
        "    type    = 'model'\n",
        ")\n",
        "\n",
        "model_artifact.description = 'Version trained on my laptop'\n",
        "\n",
        "model_artifact.add_file(\n",
        "    save_final_model_at\n",
        ")\n",
        "\n",
        "wandb.log_artifact(\n",
        "    model_artifact\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkXKeKcuXzdq"
      },
      "source": [
        "## Done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "HB8ZZlTYkxLs"
      },
      "outputs": [],
      "source": [
        "if use_of_wandb:\n",
        "  if finish_wandb_when_done:\n",
        "    from wandb  \\\n",
        "      import    \\\n",
        "      finish\n",
        "    \n",
        "    finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os \\\n",
        "    import remove\n",
        "\n",
        "from os.path \\\n",
        "    import (\n",
        "        isfile, \n",
        "        isdir\n",
        "    )\n",
        "\n",
        "from shutil \\\n",
        "    import rmtree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "if isdir(\n",
        "    load_model_from\n",
        "):\n",
        "    rmtree(\n",
        "        load_model_from\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "if isfile(\n",
        "    save_final_model_at\n",
        "):\n",
        "    remove(\n",
        "        save_final_model_at\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "if isfile(\n",
        "    retrieve_location_for_best_model()\n",
        "):\n",
        "    remove(\n",
        "        retrieve_location_for_best_model()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "if isdir(\n",
        "    location_for_tensorboard\n",
        "):\n",
        "    rmtree(\n",
        "        location_for_tensorboard\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "if shutdown_on_finish:\n",
        "  if is_google_colab_platform:\n",
        "    from google.colab \\\n",
        "      import runtime\n",
        "\n",
        "    runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "name": "Mjoelner - Training Only (Current Iteration)",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
